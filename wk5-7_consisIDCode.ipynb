{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b55d538e-9613-4252-a2fc-c0feda761f7d",
   "metadata": {},
   "source": [
    "#### 1. 人脸特征提取与融合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035b6a95-c5bf-41cf-99e6-8c7370a1af1f",
   "metadata": {},
   "source": [
    "##### 1.1 process_face_embeddings 函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c965deac-fe45-4c12-9dc9-7caff5c2770d",
   "metadata": {},
   "source": [
    "该函数调用模型提取人脸特征，\n",
    "\n",
    "（1）使用到的模型和关键参数有\n",
    "\n",
    "- face_helper_1:alignment and landmark detection.\n",
    "  \n",
    "    RetinaFace 检测bbox + 5 点 landmarks + 人脸对齐及裁剪为`[512,512]`； BiSeNet 解析19类掩码\n",
    "- face_helper_2:embedding extraction.\n",
    "\n",
    "    iResNet提取`[1,512]`的ArcFace 向量\n",
    "- eva_transform_mean & eva_transform_std: for image normalization before passing to EVA model.\n",
    "\n",
    "    将待评估图像的分布归一化到EVA-CLIP模型的输入域\n",
    "- app: Application instance used for face detection.\n",
    "\n",
    "    InsightFace人脸信息实例\n",
    "\n",
    "（2）函数的输入为RGB图像、模型、EVA-CLIP参数\n",
    "\n",
    "（3）函数的输出包括\n",
    "```\n",
    "id_cond,  # [通用特征1*512，单位化的CLIP输出语义向量1*768]，(1,1280)\n",
    "id_vit_hidden,  # CLIP最后一层的潜变量 list(torch.Size([1, 577, 1024]))\n",
    "return_face_features_image_2,  # 彩色的人脸特征图 torch.Size([1, 3, 512, 512])\n",
    "face_kps  # 来自InsightFace提取，若失败，则用Helper1(RetinaFace)提取 list(5,2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc51554-68f4-49df-aca8-71ec2b3cd8c5",
   "metadata": {},
   "source": [
    "##### 1.2  Local Face Extracter\n",
    "\n",
    "一个神经网络，融合身份信息与vit视觉特征，用其训练可学习的query，再通过和FFN调制，提取出与身份一致且具有判别力的局部人脸特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d5ef2a-748f-4a4b-8b11-cb4b1dfb3935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, id_embeds: torch.Tensor, vit_hidden_states: List[torch.Tensor]) -> torch.Tensor:  # [B,257,1024]共5个\n",
    "        # Repeat latent queries for the batch size\n",
    "        latents = self.latents.repeat(id_embeds.size(0), 1, 1)  #[B,32，1024]\n",
    "\n",
    "        # Map the identity embedding to tokens\n",
    "        id_embeds = self.id_embedding_mapping(id_embeds)  \n",
    "        id_embeds = id_embeds.reshape(-1, self.num_id_token, self.vit_dim)  # [B,1280]->[B,5,1024]，将id嵌入放到与vit_hidden同维度\n",
    "\n",
    "        # Concatenate identity tokens with the latent queries\n",
    "        latents = torch.cat((latents, id_embeds), dim=1)  # [B,32,1024]+[B,5,1024]  # 同时学习, 可能是attention难收敛.\n",
    "\n",
    "        # Process each of the num_scale visual feature inputs\n",
    "        for i in range(self.num_scale):\n",
    "            vit_feature = getattr(self, f\"mapping_{i}\")(vit_hidden_states[i])\n",
    "            ctx_feature = torch.cat((id_embeds, vit_feature), dim=1)    # [B,262,1024]，这是id—embeds和vit—feature的拼接\n",
    "\n",
    "            # Pass through the PerceiverAttention and ConsisIDFeedForward layers\n",
    "            for attn, ff in self.layers[i * self.depth : (i + 1) * self.depth]:\n",
    "                latents = attn(ctx_feature, latents) + latents # q:latents;kv_crop:ctx_feature\n",
    "                latents = ff(latents) + latents\n",
    "\n",
    "        # Retain only the query latents\n",
    "        latents = latents[:, : self.num_queries]\n",
    "        # Project the latents to the output dimension\n",
    "        latents = latents @ self.proj_out  # [B, 32, 2048]\n",
    "        return latents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbabd005-35d2-408c-be10-0372c10c930b",
   "metadata": {},
   "source": [
    "局部提取器会首先随机初始化一个可学习的latents，形状为`[1, num_queries=32, vit_dim=1024]`。接着，id_embeds通过MLP(`[B, 1280]`-> `[B,5,1024]`)变形，\n",
    "拼接到latents后面。第二部分ctx_features由id_embeds和vit_feature(`[B, 257, 1024]`)拼接而成，并通过交叉注意力融入latents。网络由10组交替的PerceiverAttention(q=context; k,v=latents)和FFN构成\n",
    "\n",
    "这一设计可追溯到[Perceiver: General Perception with Iterative Attention](arxiv.org/abs/2103.03206)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29229d87-965e-4c5e-a2c1-bf0b941e1ca8",
   "metadata": {},
   "source": [
    "#### 2. 管线"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c29f1be-19fa-4e26-b880-c6b2452f79b9",
   "metadata": {},
   "source": [
    "##### 2.1 ConsisIDTransformer3DModel\n",
    "\n",
    "前向双流DiT，以CogVideoX为基础。若为训练模式，在第偶数次经过DiT时，会训练Perceiver_cross_attn。q是hidden_states,kv是valid_face_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e388fe-b125-459c-a16f-ce6dce95e1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # 3. Transformer blocks\n",
    "        ca_idx = 0\n",
    "        for i, block in enumerate(self.transformer_blocks):\n",
    "            hidden_states, encoder_hidden_states = block(\n",
    "                hidden_states=hidden_states,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                temb=emb,\n",
    "                image_rotary_emb=image_rotary_emb,\n",
    "                attention_kwargs=attention_kwargs,\n",
    "            )\n",
    "\n",
    "            if self.is_train_face:\n",
    "                if i % self.cross_attn_interval == 0 and valid_face_emb is not None: # 在[0,2,4,...,28]\n",
    "                    hidden_states = hidden_states + self.local_face_scale * self.perceiver_cross_attention[ca_idx](\n",
    "                        valid_face_emb, hidden_states  # q: hidden_states ;kv:valid_face_emb\n",
    "                    )  # torch.Size([2, 32, 2048])  torch.Size([2, 17550, 3072])\n",
    "                    ca_idx += 1\n",
    "\n",
    "        hidden_states = self.norm_final(hidden_states)\n",
    "\n",
    "        # 4. Final block\n",
    "        hidden_states = self.norm_out(hidden_states, temb=emb)\n",
    "        hidden_states = self.proj_out(hidden_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e707fb2-40b0-4271-aff8-4facc5a9059c",
   "metadata": {},
   "source": [
    "##### 2.2 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717dac55-6162-4eec-b267-ed3f08dc3973",
   "metadata": {},
   "source": [
    "###### 2.2.1 id跟踪机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9718c5-dcc4-459b-82b3-636c23964b47",
   "metadata": {},
   "source": [
    "核心函数`find_max_confidence_bbox`会从视频的全部帧中提取出所有的人物(id)，对每个id，提取其在各帧中置信度最高的一个bbox(可能是face,head或person级别的框)，将id、best_bbox坐标及其所在帧、置信度保存在字典中，供视频分割模型SAM使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404b1ca8-a2fd-4dc5-8fb3-95839eae644e",
   "metadata": {},
   "source": [
    "`video_predictor.add_new_points_or_box()`会启动对某帧中人像的分割，接着`predictor.propagate_in_video()`会将该过程传播到视频各帧。\n",
    "除人物帧掩码、关键帧及其类型外，程序会额外保存`valid_frame`，即每个对象在这些帧被识别到。\n",
    "\n",
    "接着，在`dataloader.py`中，`get_valid_segments(valid_frame, tolerance=5)`函数将有效帧合并成若干个列表，这允许模型在一个**相对连续的片段上学习**人脸保持. `generate_frame_indices_for_face(n_frames, sample_stride, valid_frame, tolerance=7, ...)`会选取最长的一个片段，将其截断或扩展到`n_frames`，但可能会带来缓慢的运动片段。\n",
    "\n",
    "最终，使用键`track_id`跟踪视频中出现的人物。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab91121-73bd-4403-90ac-4bd34dadecb4",
   "metadata": {},
   "source": [
    "###### 2.2.2 生成训练数据————get_batch()函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece0ad85-7bd4-4730-804f-12f7708050b8",
   "metadata": {},
   "source": [
    "输入: idx\n",
    "输出：`<tuple>(pixel_values, text, 'video', video_dir, expand_face_imgs, dense_masks_tensor, selected_frame_index, reserve_face_imgs, original_face_imgs)`\n",
    "\n",
    "is_train_face 分支\n",
    "\n",
    "首先会导入4个json，分别是"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d56236-45ed-4031-aac0-b4f213d41b69",
   "metadata": {},
   "source": [
    "- `bbox_data`给出track_id、特征、所在帧、bbox的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc93ecd-c0a1-45bb-ba1e-f7addc2edfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"1\": {\n",
    "        \"frames\": [0, 1, 2],\n",
    "        \"face\": [\n",
    "            {\"frame_id\": 0, \"box\": {\"x1\": 100, \"y1\": 120, \"x2\": 150, \"y2\": 170}, \"confidence\": 0.98},\n",
    "            {\"frame_id\": 1, \"box\": {\"x1\": 102, \"y1\": 121, \"x2\": 152, \"y2\": 171}, \"confidence\": 0.97}\n",
    "        ],\n",
    "        \"head\": [\n",
    "            {\"frame_id\": 1, \"box\": {\"x1\": 90, \"y1\": 100, \"x2\": 160, \"y2\": 180}, \"confidence\": 0.99},\n",
    "            {\"frame_id\": 2, \"box\": {\"x1\": 91, \"y1\": 101, \"x2\": 161, \"y2\": 181}, \"confidence\": 0.95}\n",
    "        ],\n",
    "        \"person\": [\n",
    "             {\"frame_id\": 1, \"box\": {\"x1\": 50, \"y1\": 80, \"x2\": 250, \"y2\": 480}, \"confidence\": 0.96}\n",
    "        ]\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5704cf4a-48f8-4a44-b198-86b686db34f9",
   "metadata": {},
   "source": [
    "- `corresponding_data`将原始track_id映射到内部掩码id，形如"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdbac57-f41a-4094-b0a4-cfd693237d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"1\": {\n",
    "        \"head\": 1,\n",
    "        \"person\": 2\n",
    "    },\n",
    "    \"2\": {\n",
    "        \"face\": 3\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50be2d01-202d-44b4-92b2-3d636984e9fd",
   "metadata": {},
   "source": [
    "- `control_sam2_frame`给出track_id的特征对应的关键帧，例如，id为1的人，其头部置信度最高的框位于第1帧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f140e96-a913-455f-9bc6-7d47675dc7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"1\": {\n",
    "        \"head\": 1,\n",
    "        \"person\": 1\n",
    "    },\n",
    "    \"2\": {\n",
    "        \"face\": 1\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fbd53a-8a37-4f42-8693-ed58ac8d34b8",
   "metadata": {},
   "source": [
    "- `valid_frame`给出成功生成掩码的帧位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d59e32-7eb4-4bbf-b6b2-fc872a11d941",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"1\": {\n",
    "        \"head\": [1, 2, 3],\n",
    "        \"person\": [1, 2, 3, 4]\n",
    "    },\n",
    "    \"2\": {\n",
    "        \"face\": [1, 2]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffe2baf-aa8a-4490-8810-eb984e731624",
   "metadata": {},
   "source": [
    "接着，将`corresponding_data`保存到`<list>valid_id`中，并随机抽取一个track_id作为训练对象，确定需要抓捕的帧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0c12f8-7f77-4de4-a9af-18242004b858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get video\n",
    "                total_index = list(range(video_num_frames))  # 总帧list\n",
    "                # valid_id对应的人物(head&face)出现帧list\n",
    "                batch_index, _ = generate_frame_indices_for_face(self.max_num_frames, self.sample_stride, valid_frame[valid_id],\n",
    "                                                                          self.miss_tolerance, self.skip_frames_start_percent, self.skip_frames_end_percent,\n",
    "                                                                          self.skip_frames_start, self.skip_frames_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a762fb-7bf6-4353-bcb7-6f45786dfa5a",
   "metadata": {},
   "source": [
    "接着，调用select_mask_frames_from_index()函数处理得到\n",
    "\n",
    "- `selected_frame_index`: 使用其内部select_frames_with_distance_constraint()函数选中的帧，它会从`valid_frame`中选出间隔至少为`min_distance`的`num_frames`个帧, 并且优先选择高置信度的帧。\n",
    "- `selected_masks_dict` : 选中帧的掩码列表，掩码的生成基于对视频帧做分割得到的二值化图像序列。\n",
    "- `selected_bboxs_dict` : 选中帧的bbox列表\n",
    "- `dense_masks_dict` : 全体帧的掩码列表\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fafb7d-c5c6-462a-ac47-e5554b0daccf",
   "metadata": {},
   "source": [
    "##### 2.3 训练\n",
    "\n",
    "首先冻结无关参数。只有局部提取器、交叉注意力块、DiT每次注意力的LoRA权重被解冻，其余部分(文本编码器、VAE、DiT的其他块均被冻结)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba817b8-5f18-44d5-8437-8499eb486b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.is_train_face:  #训LFE和PCA\n",
    "        unfreeze_modules = [\"local_facial_extractor\", \"perceiver_cross_attention\"]\n",
    "\n",
    "        for module_name in unfreeze_modules:\n",
    "            try:\n",
    "                for param in getattr(transformer, module_name).parameters():\n",
    "                    param.requires_grad = True\n",
    "            except AttributeError:\n",
    "                continue\n",
    "\n",
    "        if args.is_train_lora:  # 增训lora\n",
    "            transformer_lora_config = LoraConfig(\n",
    "                r=args.rank,\n",
    "                lora_alpha=args.lora_alpha,\n",
    "                init_lora_weights=True,\n",
    "                target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],\n",
    "                # Need to check 'exclude_modules'\n",
    "                # exclude_modules=unfreeze_modules,\n",
    "            )\n",
    "            transformer.add_adapter(transformer_lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269f0cc7-de40-4a64-9b32-a9be91b33b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算 MSE loss\n",
    "if args.is_train_face and len(valid_indices) == 0:\n",
    "    # 无有效样本，直接返回 0\n",
    "    loss = torch.zeros((), device=model_pred.device, dtype=model_pred.dtype)\n",
    "else:\n",
    "    loss = (weights * (model_pred - target) ** 2).reshape(batch_size, -1)\n",
    "\n",
    "    # 仅在训练人脸且开启 mask 时才做掩膜，再求平均\n",
    "    if args.is_train_face and args.enable_mask_loss and enable_mask_loss_flag:\n",
    "        loss = (loss * dense_masks).sum() / dense_masks.sum()\n",
    "    # 否则直接平均，得到：0(no train face), batch_loss(train face), batch_loss_with_mask(train face+mask+mask_flag)\n",
    "    else:\n",
    "        loss = loss.mean(dim=1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30626ec6-4bd3-4766-aa48-5b5f9e5199f0",
   "metadata": {},
   "source": [
    "损失函数如上。其中target是无噪声的video_latents，model_pred是根据时间步预测的去噪结果。最后在batch维度平均。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641e11af-94fd-444c-90d2-f22798b96c35",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 3. 评估"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8654bf30-78bd-4586-a761-42fb5385d913",
   "metadata": {},
   "source": [
    "##### 3.1 CLIP得分\n",
    "\n",
    "使用`cap.set()`跳转到视频文件的指定帧, `cap.read()`提取帧, 将原视频等距采样16帧，保存到一个列表`frames`中，并计算这16张图片关于同一prompt的CLIP得分之平均。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9306100-e380-43a0-9c37-bc3dec34771d",
   "metadata": {},
   "source": [
    "##### 3.2 Arc得分, Cur得分, FID得分\n",
    "\n",
    "分别衡量ref和视频帧之间在\n",
    "\n",
    "- ArcFace模型、CurricularFace模型特征空间的余弦相似度\n",
    "- FID得分衡量InceptionV3特征空间中的距离。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DiffLab]",
   "language": "python",
   "name": "conda-env-DiffLab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
