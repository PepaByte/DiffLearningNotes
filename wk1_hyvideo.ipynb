{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31d9bcfd-855b-4894-92a9-10c8985a3c0c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### HunyuanVideoPipiline类\n",
    "\n",
    "其`__call__`方法主要有以下流程:\n",
    "\n",
    "(1)check_inputs检查输入\n",
    "\n",
    "(2)准备输入\n",
    "\n",
    "- text_embeddings(使用encode_prompt())\n",
    "- timestep(使用retrieve_timesteps)\n",
    "- latents(使用prepare_latents)\n",
    "\n",
    "(3)去噪(denoising_step循环)\n",
    "\n",
    "(4)解码输出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad58aae-bd99-4dae-8578-88d796d1a954",
   "metadata": {},
   "source": [
    "1. text_embeddings\n",
    "\n",
    "有`prompt_embeds, negative_prompt_embeds, prompt_mask, negative_prompt_mask`作为输出。\n",
    "\n",
    "如果使用CFG，则会将其concat成`[prompt_embeds, negative_prompt_embeds]`,mask同理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734f47e8-a479-43ca-9226-5927d68c8de7",
   "metadata": {},
   "source": [
    "2. timestep\n",
    "\n",
    "输出递降的`<list>` `timesteps`和`<int>` `num_inference_steps`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5054d6b-50fc-4322-b042-b42c6245dc37",
   "metadata": {},
   "source": [
    "3. latents\n",
    "\n",
    "会先将`video_length`做$(L-1)//4+1$，接着将`video_length = 33` `num_channels_latents = 16`送入`prepare_latents()`.在该函数中，若\n",
    "- 启用`latent_concat`(默认关闭),则会生成`[B,C,F,H,W]=[1,7,33,24,42]`的`latents`\n",
    "- 启用`i2v_stability`(默认开启),则会线性注入`0.001`的`img_latents`,得到`[B,C,F,H,W]=[1,16,33,24,42]`的`latents`\n",
    "- 启用`FlowMatching`调度器且有`init_noise_sigma`属性，则会把`latents`的方差变为$init\\_noise\\_sigma^2 * I$\n",
    "\n",
    "如果启用`latent_concat`，`img_latents_concat`会是形如`[1,16,33,24,42]`,第一帧存在，后续帧全为0的`<tensor>`,`i2v_mask`是第一元1，后续0的`[33,]`张量，`mask_concat`是形如`[1,1,33,24,42]`，第一帧全为1,后续帧全为0的张量."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9631ea-9d1d-480b-9b91-f5445b76ddb1",
   "metadata": {},
   "source": [
    "4. denoising step\n",
    "\n",
    "使用`for i, t in enumerate(timesteps)`循环\n",
    "- 若启用`latent_concat`,`latent_model_input`会是`latents, img_latents_concat, mask_concat`按**通道**维拼接的结果，形如`[1,7+16+1,33,24,42]`,否则为`[1,16,33,24,42]`的`latents`\n",
    "- 若启用`token_replace`,在去噪前，每个时间步的第一帧都将被`img_latents`替换，去噪只在后续帧执行，去噪后会再与`img_latents`拼接。这一重复可能是为了便于回调。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ce7f2d-f6c7-4fd4-a85d-fc6775a403a5",
   "metadata": {},
   "source": [
    "5. decode\n",
    "\n",
    "$latents = latents/vae\\_sacling\\_factor+vae\\_shifting\\_factor$,\n",
    "\n",
    "接着将其通过VAE，得到`image`。将其从`[-1,1]`转换到`(0,1)`,即作为`HunyuanVideoPipelineOutput`的属性."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a89f220-25a4-4c0c-9872-9a3f3b911e59",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### HunyuanVideoSampler类\n",
    "\n",
    "其`predict`方法主要实现以下功能，并最后输出一个`out_dict`：\n",
    "\n",
    "(1)初始化随机种子和FlowMatchDiscreteScheduler\n",
    "\n",
    "(2)根据图片高宽比选择适配的视频尺寸,并裁剪图片\n",
    "\n",
    "(3)调用pipeline完成inference\n",
    "\n",
    "注：参数ulysses或ring会启用并行VAE编码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33f24fb-a929-455c-a702-910c183a01ca",
   "metadata": {},
   "source": [
    "1. seeds\n",
    "\n",
    "将不同类型的种子输入统一为`batch_size * num_videos_per_prompt`个种子的`<list>` `seeds`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbe6d59-dccb-4bbc-801f-2672519b0a39",
   "metadata": {},
   "source": [
    "2. crop\n",
    "\n",
    "根据所选清晰度，依据最邻近高宽比得到和原图片尺寸最匹配的`closest_size`和`closest_ratio`，用于裁剪。接着`[3,H,W]`的RGB图像形状会被缩放、裁剪，接着值被缩放到`[-0.5,0.5]`, unsqueeze维度到`[1,3,1,H,W]`,\n",
    "\n",
    "接着，经VAE编码，再乘系数$vae\\_scaling\\_factor$,得到`img_latents`.图片可经`<list[uint8]>` `semantic_images`传入，或只使用图片路径导入."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094214ac-4984-4f38-8754-d50f0dc61d55",
   "metadata": {},
   "source": [
    "### VAE\n",
    "\n",
    "其中`EncoderCausal3D`和`DecoderCausal3D`分别实现VAE的主干部分，它们依赖于UNet中的组件.`AutoencoderKLCausal3D`类的`encode`和`_decode`方法调用这两个神经网络.`tiling,slicing,fuseqkv,attn`是几个额外功能,其中只有`vae_tiling=True`,故只考虑这部分."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7f0d6e-19c9-490a-85e9-49c1d1a8fced",
   "metadata": {},
   "outputs": [],
   "source": [
    "[VAE编码器结构]<img src=\"/imgs/wk1/encoder.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2271ed-f1fb-459f-b377-516a18056c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "[VAE解码器结构]<img src=\"/imgs/wk1/decoder.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1559435-f614-4015-b514-d4f9c24c3a24",
   "metadata": {},
   "source": [
    "\"the tiles overlap and are blended together to form a smooth output\"这个做法很工程！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aa9e32-cb87-4a7d-92a0-f723ceb10dd9",
   "metadata": {},
   "source": [
    "1. spatial_tiled_encode()\n",
    "\n",
    "它会把图像分割成小块，装入二元列表`rows`中，分别编码，然后拼接. 其中有关键参数\n",
    "- `tile_sample_min_size`: 默认为32,将图像按空间维度裁剪为方体，但最后一行/列不方\n",
    "- `tile_overlap_factor`: 重叠比例，默认为0.25\n",
    "- `overlap_size`: 相邻 tile 起点之间的步长\n",
    "- `blend_extent`: 需要融合的宽度/高度\n",
    "- `row_limit`: 切片边缘,使用时为`[:,:,:,:row_limit,:row_limit]`. 每次融合其左/上方的图片，并在存储时去掉其右侧、下侧`blend_extent`宽度的像素\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6a7005-93e3-47cf-8fd4-bba0fccd464c",
   "metadata": {},
   "source": [
    "2. temporal_tiled_encode()\n",
    "\n",
    "它会按时间维度将图像分成小块，并在每个时间块递归地调用`spatial_tiled_encode()`, 待空间编码融合后, 再进行时间编码、融合。解码部分待补充。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e73b689-96c7-4cb4-ab9e-32b65caec48a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### UNet\n",
    "\n",
    "包含的模块(类)有\n",
    "\n",
    "- 卷积块`CausalConv3d`\n",
    "- 基于它的`DownsampleCausal3D`和`UpsampleCausal3D`\n",
    "- 以及进一级抽象`DownEncoderBlockCausal3D`和`UpEncoderBlockCausal3D`(ModuleList)\n",
    "- 此外，还有`ResnetBlockCausal3D`和`UNetMidBlockCausal3D`(ModuleList)\n",
    "\n",
    "需要注意的是，上采样时没有使用`ConvTranspose`,而是使用了先插值、再卷积的模式，这可能是由于反卷积的设计无法实现“因果性”(即后续帧不影响前面帧)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2979e769-f671-4a80-8c73-12c08c7f09cf",
   "metadata": {},
   "source": [
    "1. CausalConv3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be8c47e4-deb3-4798-a262-94be84b3bb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "        x = F.pad(x, self.time_causal_padding, mode='replicate')\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1192bf7a-a5ee-4a98-894f-346ed2540a70",
   "metadata": {},
   "source": [
    "其中，`<tuple>` `self.time_causal_padding = (kernel_size // 2, kernel_size // 2, kernel_size // 2, kernel_size // 2, kernel_size - 1, 0)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e829606-e1e2-409f-8df9-e02a2329ca50",
   "metadata": {},
   "source": [
    "[WaveNet](https://arxiv.org/abs/1609.03499)中提到，针对图像卷积，可以使用掩码卷积来实现因果性。但此处使用了和原论文类似的填充方式。举个例子，一个在T维度上原先为`<x0,x1,...,xn>`的序列，如果用`kernel=3`的卷积，将会使用`<x0,x1,x2>`,`<x1,x2,x3>`,...作为输入,这会使用后续时间的信息。左边先补两格，`<P,P,x0,x1,...>`,将会使用`<P,P,x0>`,`<P,x0,x1>`,`<x0,x1,x2>`作为输入，实现因果性。这里`P<- x0`.\n",
    "\n",
    "注：`attention masking fill`似乎与此类似\n",
    "\n",
    "注：`padding`的格式是上、下、左、右、前、后，分别对应`H,W,T`三个维度.空间维度padding是为了保持输入输出形状不变"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c2d67a-db10-44df-bd9c-507b53c9d4ca",
   "metadata": {},
   "source": [
    "2. UpsampleCausal3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8f15e31-b557-42ac-b4d8-0e8bae1442f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, Union\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "def forward(\n",
    "        self,\n",
    "        hidden_states: torch.FloatTensor,\n",
    "        output_size: Optional[int] = None,\n",
    "        scale: float = 1.0,\n",
    "    ) -> torch.FloatTensor:\n",
    "    #此前省去...\n",
    "    if self.interpolate:  # 分离第一帧，分别插值\n",
    "        B, C, T, H, W = hidden_states.shape\n",
    "        first_h, other_h = hidden_states.split((1, T - 1), dim=2)\n",
    "        if output_size is None:\n",
    "            if T > 1:\n",
    "                other_h = F.interpolate(other_h, scale_factor=self.upsample_factor, mode=\"nearest\")  # 4n+1的设计导致这里必须拆出一帧。那任何一帧都OK吗？似乎不是。\n",
    "\n",
    "            first_h = first_h.squeeze(2)\n",
    "            first_h = F.interpolate(first_h, scale_factor=self.upsample_factor[1:], mode=\"nearest\")\n",
    "            first_h = first_h.unsqueeze(2)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        if T > 1:\n",
    "            hidden_states = torch.cat((first_h, other_h), dim=2)\n",
    "        else:\n",
    "            hidden_states = first_h\n",
    "\n",
    "        # If the input is bfloat16, we cast back to bfloat16\n",
    "        if dtype == torch.bfloat16:\n",
    "            hidden_states = hidden_states.to(dtype)\n",
    "\n",
    "        if self.use_conv:  #插值完再conv\n",
    "            if self.name == \"conv\":\n",
    "                hidden_states = self.conv(hidden_states)\n",
    "            else:\n",
    "                hidden_states = self.Conv2d_0(hidden_states)\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d5ce3e-7f9e-4db1-aa80-560eef521375",
   "metadata": {},
   "source": [
    "这里`<tuple>` `self.upsample_factor=(2, 2, 2)`,`self.conv=CausalConv3d(self.channels, self.out_channels, kernel_size=3, bias=bias)`,默认`stride=1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082a0658-e43f-4020-ad01-d7c0feb30a86",
   "metadata": {},
   "source": [
    "3. DownsampleCausal3D\n",
    "\n",
    "   与UpsampleCausal3D类似，但直接使用卷积降采样."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bea614b1-33d6-482c-adeb-60f1566f21ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, hidden_states: torch.FloatTensor, scale: float = 1.0) -> torch.FloatTensor:\n",
    "        assert hidden_states.shape[1] == self.channels\n",
    "\n",
    "        if self.norm is not None:\n",
    "            hidden_states = self.norm(hidden_states.permute(0, 2, 3, 1)).permute(0, 3, 1, 2) \n",
    "            # B C H*W T → B H*W T C → B C H W (T是何时不见的？似乎是bug,但由于默认不启用norm,故无报错)\n",
    "        assert hidden_states.shape[1] == self.channels\n",
    "\n",
    "        hidden_states = self.conv(hidden_states)  # stride = 2\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfa1c9e-2dd5-4314-96eb-0ea43f9d7c7a",
   "metadata": {},
   "source": [
    "3. DownEncoderBlockCausal3D/UpDecoderBlockCausal3D\n",
    "\n",
    "`num_layers`个`ResnetBlockCausal3D`+ 1个`DownsampleCausal3D`/`UpsampleCausal3D`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d0326f-7cec-4da5-9157-e8c06929f061",
   "metadata": {},
   "source": [
    "4. UNetMidBlockCausal3D\n",
    "\n",
    "   有一些初始参数\n",
    "\n",
    "- `attention_head_dim`: 每个头的维数，如果没有则等于`in_channels`\n",
    "- `num_layers`: 层的个数。`ModuleList`中会交替添加`num_layers`个`attn+resnet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3da5be98-fb43-4e82-b34d-71d665fa585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, hidden_states: torch.FloatTensor, temb: Optional[torch.FloatTensor] = None) -> torch.FloatTensor:\n",
    "    hidden_states = self.resnets[0](hidden_states, temb)\n",
    "    for attn, resnet in zip(self.attentions, self.resnets[1:]):  # 初始化的第一个resnet实际不会用到，只是设计考量\n",
    "        if attn is not None:\n",
    "            B, C, T, H, W = hidden_states.shape\n",
    "            hidden_states = rearrange(hidden_states, \"b c f h w -> b (f h w) c\")  # rearrage是为了便于生成因果掩码, (掩码从下一帧第一个token开始，此前为0,此后为-inf)\n",
    "            attention_mask = prepare_causal_attention_mask(\n",
    "                T, H * W, hidden_states.dtype, hidden_states.device, batch_size=B\n",
    "            )\n",
    "            hidden_states = attn(hidden_states, temb=temb, attention_mask=attention_mask)  # temb被用在spatial_norm里\n",
    "            hidden_states = rearrange(hidden_states, \"b (f h w) c -> b c f h w\", f=T, h=H, w=W)\n",
    "        hidden_states = resnet(hidden_states, temb)  # temb被用在多个norm和time_emb_proj里,并参与残差lian'jie\n",
    "        \n",
    "    return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4762c3ac-428f-4a5c-838e-d33ccde976bd",
   "metadata": {},
   "source": [
    "5. ResnetBlockCausal3D\n",
    "\n",
    "pipeline是`norm1 ->activate ->causalConv1 ->norm2 ->(Optional)+temb ->dropout ->causalConv2 ->(Optional)shortcutConv ->resConnection`,需要额外关注的点是\n",
    "\n",
    "- time_embedding\n",
    "  - 如果`time_embedding_norm`是`\"ada_group\"`或`\"spatial\"`,则`norm1`和`norm2`接受`temb`作为参数。(`norm2`需经`temb_proj`投影)\n",
    "  - 接着,若`temb_channels`不为`None`，且`time_embedding_norm`是`\"default\"`或`\"scale_shift\"`,同时`time_emb_proj`存在，并且`skip_time_act=False`,\n",
    "  - 则`temb`会被投影至与`hidden_states`相同形状\n",
    "- shortcutConv\n",
    "\n",
    "  会在`in_channels != out_channels`时启用，`kernel=stride=1`, 用于将`input`的通道数卷到和处理后的`hidden_states`相同维数,用于残差连接"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd45efdb-ba07-42ca-81f6-75f31a5d9f9a",
   "metadata": {},
   "source": [
    "### 几个待处理的问题\n",
    "\n",
    "1. 搞清楚`layer_norm`,  `group_norm`, `spatial_norm`, `batch_norm`分别是什么？有什么应用场景？使用的原因分别是什么？\n",
    "2. 搞清楚UNet中`channels`的变化\n",
    "3. 搞清楚`time_embedding`是如何参与到`norm`中的，以及还出现在除了残差块中的何处\n",
    "4. 搞清楚UNet中cross_attention出现的位置、输入输出的数量和形状\n",
    "5. 寻找上采样不使用反卷积的原因\n",
    "6. 寻找`latent_concat`在通道维度拼接的原因"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
