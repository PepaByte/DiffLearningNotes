{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c69603aa-cbc6-4b31-a3fa-72d9b8aacc42",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 视频生成中的一致性困难\n",
    "\n",
    "包括\n",
    "- 面部动作控制时的ID泄露(意外换脸)\n",
    "- 角色外观的一致性\n",
    "- 服装动力学与纹理一致性\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872fc628-7811-4590-971f-ac85056f18a1",
   "metadata": {},
   "source": [
    "#### 过去的解决方案\n",
    "\n",
    "1. ID Preserve方面\n",
    "\n",
    "- 对已有的文生图生成框架**调优**，生成ID匹配的内容 \n",
    "  - Dreambooth 全网络微调，预训练、\n",
    "  - LoRA 部分参数微调，预训练\n",
    "  - Textual Inversion 训练嵌入器学习身份表征\n",
    "\n",
    "  它们都需要对每个新身份进行微调\n",
    "\n",
    "- Tuning Free(不更新主网络参数，只训练嵌入和cross-Attention块的方法)\n",
    "\n",
    "  visual tokens guidance\n",
    "\n",
    "  - ID-Adapter利用CLIP学得的特征，通过交叉注意力*引导*模型生成身份保持的图像\n",
    "  - InstantID将CLIP特征替换为Arcface特征，融合了一个*姿态网络*\n",
    "\n",
    "  text token guidance\n",
    "\n",
    "  - PhotoMaker 将CLIP特征与text embeds *concat*\n",
    "  - Imagine yourself 逐元素加法\n",
    "\n",
    "视频领域，支持IPT2V的只有\n",
    "\n",
    "  - MovieGen(闭源)\n",
    "  - ID-Animator(使用类似图像的方法，但效果不佳)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7c5966-0afa-4ea4-ae48-fe5a29a004e4",
   "metadata": {},
   "source": [
    "### HYVideo的解决方案\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3084a3ec-35d2-4912-8b2c-97633f55d1e2",
   "metadata": {},
   "source": [
    "#### 基本思路\n",
    "\n",
    "HYVideo提供了几种条件注入的思路，其中方法2特别针对ID leak、cross-ID misalignment 和外观不一致两种现象.\n",
    "\n",
    "1. latent concat\n",
    "\n",
    "    将ref复制到合适维度后与noise沿特征维度拼接，提供强引导\n",
    "   \n",
    "2. **潜在表达+蒙版的交叉注意力调制** \n",
    "\n",
    "   由于直接使用面部关键点(facial landmarks)会导致ID leak，混元团队使用[VASA](https://arxiv.org/abs/2404.10667)提取的隐式(implicit)特征来注入，确保ID、表情的一致性。它们与视频潜变量$h$是不同维的\n",
    "\n",
    "   $$h_{i}+\\mathrm{Attn}_{\\mathrm{exp}}(h_{i},z_{\\mathrm{exp}},z_{\\mathrm{exp}})*\\mathcal{M}_{\\mathrm{face}}$$\n",
    "\n",
    "3. (只是动作引导的方案)与noise直接相加做初始引导\n",
    "\n",
    "   $$\\hat{z_t} + z_{pose}$$\n",
    "   该方法用于注入Dwpose+3DVAE提取的动作特征，由于其与原始模型共享VAE，潜空间相同，故相加时维度是匹配的，但动机存疑。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59ba274-1f6a-4567-b0b9-6a321e33ca28",
   "metadata": {},
   "source": [
    "![混元注入](./img/hyvideo_inject.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbdebbf-9905-482e-92ac-386a1439e475",
   "metadata": {},
   "source": [
    "### ConsisID的解决方案"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9e64d0-f94d-4728-85ae-d0b09d708472",
   "metadata": {},
   "source": [
    "![ConsisID框架](./img/consisid_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3c540f-b5a2-44fa-a9f8-a03939ac6c74",
   "metadata": {},
   "source": [
    "#### 基本思路\n",
    "\n",
    "1. 针对ID leak，作者基于两点**观察**：\n",
    "\n",
    "    **观察1：浅层Transformer学得的低频特征对于像素级预测(即去噪)是关键的**\n",
    "\n",
    "    **观察2：DiT对高频信息的感知能力有限**\n",
    "\n",
    "\n",
    "    以CogVideoX-5B为基线模型，通过提取人像信息的高频、低频部分，将它们分别注入DiT进行微调。同样使用带蒙版的交叉注意力调制向DiT注入图像的高频信息(保留了更多的身份信息)，使模型能捕捉更多ID信息。\n",
    "\n",
    "    基于观察2，在模型训练初期保留高频信息是低效的，冗余信息还会阻碍模型收敛。因此，作者采用渐进式训练，先只学习低频信息，再学习高频部分。\n",
    "   \n",
    "    做法上，首先提取面部关键点，转换为RGB图像，并与ref拼接，再与噪声$z_0$拼接，在扩散过程中注入低频信息(主要是外观特征，例如面部比例、表情)。接着，高频信息(不随年龄、妆容等改变，例如眼睛和唇部纹理)通过一个人脸识别主干网络 (如 ArcFace) 来提取，并与经CLIP处理得到的语义特征连接，最后使用QFormer融合这些特征，最后通过位于Attention块内部的交叉注意力调制注入DiT。CLIP encoder的使用是为了实现编辑能力(语义信息对于编辑是必要的)。实验表明，不经CLIP能够成功提升身份一致性，但缺乏*编辑所需的语义特征（semantic features）*，这类任务需要例如变更一个人的*年龄*、*妆容*。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbd3d83-3910-4d7a-b14e-18e966ac705c",
   "metadata": {},
   "source": [
    "#### 关键动作\n",
    "\n",
    "1. 高频信息注入\n",
    "\n",
    "   $$Z_i^{\\prime}=Z_i+\\mathrm{Attention}(Q_i^v,K_i^f,V_i^f)$$\n",
    "    $Z$代表已经过自注意力的Visual Token，$Q$由$Z$发起，而$K$和$V$由高频ID信息发起，$V$被注入以提升高频表征。$i$代表注意力层数。获取高频ID信息时，QFormer会随机丢弃来自CLIP的表征，这是由于CLIP并非为人像数据专门训练，可能包含了许多非面部信息。在多种注入方式的比较中，尝试了高频信息的cross-attention调制分别位于MMDiT的：入口 (f)、Attention和FFN之间 (c)、出口 (e)。实验表明：\n",
    "\n",
    "    - 在入口 (f) 处调制会导致DiT输出的预期(输入)频域变化，造成梯度爆炸\n",
    "    - 在Attention和FFN之间 (c) 比仅在出口 (e) 处调制，保留了更多高频信息(见频域分析图c和e)。至于原因，作者认为Attention块内秉地适于低频信息加工，在FFN出口处直接添加并不能有效弥补其缺点。个人认为这一比较表明，主要问题的确发生在**Attention计算对低频信息的捕获不足，而非FFN**。\n",
    "   ![信息注入方式的比较](./img/consisid_inject.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c8ddff-d20f-40c1-85ed-51d55502377d",
   "metadata": {},
   "source": [
    "#### 训练细节\n",
    "\n",
    "1. 随机掩码，防止背景生成不自然\n",
    "\n",
    "    一方面，为了集中训练模型对人物身份一致性的保持，训练噪声被掩码集中在人脸区域，以排除其余特征的干扰。但这会导致生成视频中背景信息的丢失。为了缓解这一现象，训练时掩码被以$\\alpha=0.5$概率丢弃。\n",
    "\n",
    "2. 随机跨脸训练，防止模型直接使用帧内的脸，泛化不足\n",
    "\n",
    "    训练时仅使用已在训练目标中出现的人脸作为ref，会导致模型倾向于直接使用ref替换来达成一致性保持，造成过拟合。为了缓解这一现象，训练时以$\\beta=0.5$概率采用训练帧以外的人脸作为ref。同时，ref均被引入轻微的高斯噪声(可能是逐元素乘法)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ac56fa-36ac-4090-8b50-2d61e4208e88",
   "metadata": {},
   "source": [
    "#### 实验细节\n",
    "\n",
    "1. 频域分析显示低频+关键点+高频联合注入的方法在视频中有效保留了更多的信息，同时提升高频和低频表现\n",
    "2. 移除全局特征提取器会导致最差的结果，并导致模型收敛困难，这验证了观察1 (见原文表2第一行)\n",
    "3. 消融实验表明，关键点特征图能够提升模型对低频信息的学习效果 (比较表3的cd两行)\n",
    "4. CLIP编码器的引入可以显著提升CLIPScore，即提升视频内容中成功嵌入的语义信息量 (见附录表4第一行)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "7de0b945-59e4-4517-b6a7-83c2120c73d1",
   "metadata": {},
   "source": [
    "<img src="./img/consisid_fourier.png" alt= "consisID视频频域信息分析" style="width: 200px;" align="center"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81ca4cf-ab07-4a9c-8d41-4de7ff254a53",
   "metadata": {},
   "source": [
    "### 总结\n",
    "\n",
    "1. 经验\n",
    "\n",
    "    (1) 比较两个模型的框架，我们发现latent concat能确保条件注入，不可省去。\n",
    "\n",
    "    (2) **交叉注意力调制可能是往视频DiT中补充信息的重要通用方式。**\n",
    "2. 资源消耗\n",
    "\n",
    "    该方法至少需要训练：一致性特征提取器、CrossAttention的QKV矩阵\n",
    "\n",
    "3. 一些遗留问题\n",
    "\n",
    "   - 了解QFormer\n",
    "   - ConsisID展望指出，视频一致性的评估方法有待改进。现有方法关注何方面？\n",
    "   - 阅读关于Transformer频域分析的几篇文章；了解对图像的傅里叶变换"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DiffLab]",
   "language": "python",
   "name": "conda-env-DiffLab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
